Lab Assignment 1

In this assignment, you're going to experiment with a real life armadillo sculpture scanned using a Cyberware 3030 MS 3D scanner at Stanford University. The sculpture is available as part of their 3D Scanning Repository, and is a very dense 3D mesh consisting of 172974 vertices! The mesh is available for you, located at /Module4/Datasets/stanford_armadillo.ply in the course github repository. It is not a Python script file, so don't attempt to load it with a text editor!
![](https://prod-edxapp.edx-cdn.org/assets/courseware/v1/f83af0bb88f3f2b258d91f9575c1cc07/asset-v1:Microsoft+DAT210x+1T2018a+type@asset+block/M5L3-Armadillo.png)


Start up Jupyter and open up the Module4/Module4 - Lab1.ipynb starter code in your browser, then read through it carefully. You will notice the use of a new library, Plyfile. This library loads up the 3D binary mesh for you. The mesh is further converted into a Pandas dataframe for your ease of manipulation.

Before changing any of the code, go ahead and just run through the notebook. You should see the 3D armadillo being rendered. It might take a while, considering its resolution. Your goal is to reduce its dimensionality from three to two using PCA to cast a shadow of the data onto its two most significant principal components. Then render the resulting 2D scatter plot. All of the detailed steps needed to accomplish this are listed inside of the notebook.

------------------------------

Lab Assignment 2

In Lab Assignment 1, you applied PCA to a dataset generated by 3D-scanning an actual sculpture. Real life 3D objects are a good segue to PCA, since it's fun to see its effects on a dataset we can visualize and touch. Another benefit is that all three spatial dimensions, x, y, and z, each measure the same unit-type relative (length), so no extra consideration need be made to account for PCA's weakness of requiring feature scaling.

But now the fun is over. Gaining some practical experience with real-world datasets, which rarely allot you the luxury of having features all of the same scale, will help you see how critical feature scaling is to PCA. In this lab, you're going to experiment with a subset of UCI's Chronic Kidney Disease data set, a collection of samples taken from patients in India over a two month period, some of whom were in the early stages of the disease. There is some starter code for you available in /Module4/Module4 - Lab2.ipynb.

Start the lab by looking through the attribute information on the dataset website. Whenever you are given a dataset, the first thing you should do is find out as much about it as possible, both by reading up on any metadata, as well as by prodding through the actual data. Particularly, pay attention to what the docs say about these three variables: bgr, rc, and wc.

After that, go ahead and load up the kidney_disease.csv dataset from the /Module4/Datasets/ directory. Follow the rest of the steps and reading in the notebook to complete the assignment. Pay special attention to the dataframe's dtypes as assigned by Pandas. You may have to coerce a column into a different data type, if necessary. The last thing you'll do is reduce your dataset down to two principal components by running it through PCA, and then visualizing the resulting output.

---------------------------

Lab Assignment 3

You're not quite done with chronic kidney disease yetâ€”we still need to beat it! In the previous lab assignment, you focused only on three features out of the entire dataset: bgr, rc, and wc. That should have seemed strange to you. How did we know to direct your attention only to those features? The answer, of course, is through PCA. By running PCA on the raw dataset data, we were able to find suitable candidate features to show the importance of feature scaling. For this lab, there will be no starter code. Copy your finished Lab 2, Module4 - Lab2.ipynb file over as Module4 - Lab3.ipynb and start working from that.

Head back over to the dataset page (or you can look at the kidney_disease.names file in your /Module4/Datasets/ directory). Each column has a type listed, e.g. numeric, nominal, etc. We've included a formatted list of the nominal features below. Instead of using an indexer to select just the bgr, rc, and wc, alter your assignment code to drop all the nominal features, which we've listed below for your copy and pasting pleasure. Be sure you select the right axis for columns and not rows, otherwise Pandas will complain! ['id', 'classification', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane']
Print out your dataset's dtypes, inspect the results. Does everything look like it should / properly numeric encoded? If not, make code changes to coerce the remaining column(s).
Run your assignment and then answer the questions below.

------------------------------

Lab Assignment 4

After having a brief conversation with Joshua Tenenbaum, the primary creator of the isometric feature mapping algorithm, it only seems right that we make your first lab assignment be replicating his canonical, dimensionality reduction research experiment for visual perception! In fact, you will also be using his original dataset (cached website) from December 2000. It consists of 698 samples of 4096-dimensional vectors. These vectors are the coded brightness values of 64x64-pixel heads that have been rendered facing various directions and lighted from many angles. Replicate Dr. Tenenbaum's experiment by:

- Applying both PCA and Isomap to the 698 raw images to derive 2D principal components and a 2D embedding of the data's intrinsic geometric structure.
- Project both onto a 2D scatter plot, with a few superimposed face images on the associated samples.
- Extra: If you're feeling fancy, increase n_components to three, and plot your scatter plot on a 3D chart.
